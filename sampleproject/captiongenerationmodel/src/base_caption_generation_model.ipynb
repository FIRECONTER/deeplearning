{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: \\n    1) Use the pre handled data(image description and image features to train the caption geneartion model\\n    2) firstly quick choose a model to train\\nAuthor: allocator\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Description: \n",
    "    1) Use the pre handled data(image description and image features to train the caption geneartion model\n",
    "    2) firstly quick choose a model to train\n",
    "Author: allocator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "import numpy.random as rd\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dir = '../data/img'\n",
    "img_feature_dir = '../data/img_feature'\n",
    "img_feature_file = 'image_features.h5'\n",
    "clean_txt_dir = '../data/clean_txt'\n",
    "clean_txt_file = 'image_descs.json'\n",
    "set_category = 200\n",
    "seed = 10\n",
    "output_dir = '../data/res'\n",
    "output_filename = 'development_dataset_id.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare the image feature list and the image desc list\n",
    "def compare_feature_desc(feature_file, desc_file):\n",
    "    \"\"\"Compare the image feature and desc.\"\"\"\n",
    "    image_features = h5py.File(feature_file, 'r')\n",
    "    image_descs = json.load(open(desc_file, 'r'))\n",
    "    image_feature_keys = dict(image_features.keys())\n",
    "    image_descs_keys = image_descs.keys()\n",
    "    print(' current image_feature_keys')\n",
    "    print(image_feature_keys)\n",
    "    print(' current image_descs_keys')\n",
    "    print(image_descs_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first seperate the data set\n",
    "def seperate_dataset(img_dir, category, seed):\n",
    "    \"\"\"Generate the development set to quickly choose the model and configuration.\"\"\"\n",
    "    img_list = os.listdir(img_dir)\n",
    "    # random select the train set and test set from the image list\n",
    "    img_list = [item.split('.')[0] for item in img_list]\n",
    "    img_size = len(img_list)\n",
    "    extract_set = set()\n",
    "    dataset = {}\n",
    "    rd.seed(seed)\n",
    "    while len(extract_set) < category:\n",
    "        curr_id = rd.randint(img_size)\n",
    "        curr_item = img_list[curr_id]\n",
    "        if curr_item not in extract_set:\n",
    "            extract_set.add(curr_item)\n",
    "    print(' extract set generated and length %d ' % len(extract_set))\n",
    "    set_len = int(category/2)\n",
    "    dataset['train'] = list(extract_set)[:set_len]\n",
    "    dataset['test'] = list(extract_set)[set_len:]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the dataset id\n",
    "def save_dataset(filename, dataset):\n",
    "    \"\"\"Save the dataset.\"\"\"\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    json.dump(dataset, open(file_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the text and each photo has one complete sentence\n",
    "def load_txt(filename, dataset):\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    train_id_list = dataset['train']\n",
    "    test_id_list = dataset['test']\n",
    "    image_descs = json.load(open(filename, 'r'))\n",
    "    for item in train_id_list:\n",
    "        train_set[item] = 'startseq ' + ' '.join(image_descs[item]) + ' endseq'\n",
    "    for item in test_id_list:\n",
    "        test_set[item] = 'startseq ' + ' '.join(image_descs[item]) + ' endseq'\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the features about the images\n",
    "def load_image_feature(filename, dataset):\n",
    "    \"\"\"Load the image feature about the train and test image id list.\"\"\"\n",
    "    image_features = h5py.File(filename, 'r')\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    train_id_list = dataset['train']\n",
    "    test_id_list = dataset['test']\n",
    "    for item in train_id_list:\n",
    "        train_set[item] = np.array(image_features[item])\n",
    "    for item in test_id_list:\n",
    "        test_set[item] = np.array(image_features[item])\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to encode the descriptions before training\n",
    "# from words to unique integer values\n",
    "# analyze the Tokenizer and fit_on_texts\n",
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"Encode the descriptions to numbers for model training.\"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    # each line contains the the description sentence about the image\n",
    "    lines = list(descriptions.values())\n",
    "    # 0 is a reserved index so the word's index starts with 1\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the important generate training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the sequence of the images input sequences and output words for an image\n",
    "def create_sequences(tokenizer, image_desc, image, max_length):\n",
    "    ximage, xseqs, y = list(), list(), list()\n",
    "    # encode the description with integer\n",
    "    # pluse one because the index of 0 is reversed \n",
    "    # when calculate the vocab_size should add the reversed 0\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    sequence = tokenizer.texts_to_sequences([image_desc])[0]\n",
    "    # splite current sequence to multiple x,y pairs\n",
    "    for i in range(1, len(sequence)):\n",
    "        # split the sequence\n",
    "        input_seq, output_seq = sequence[:i], sequence[i]\n",
    "        # pad input sequence to make the sequence same length\n",
    "        # max length is used to limit the input sequences\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
    "        # encode output make the output as a categorical list\n",
    "        output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "        ximage.append(image)\n",
    "        xseqs.append(input_seq)\n",
    "        y.append(output_seq)\n",
    "    # each picture contains seqs images output are list\n",
    "    return [ximage, xseqs, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some explain about the model\n",
    "# the structure of the model and the design of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# begin to fit the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    # image feature map\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs image descs sequence and image features\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # plot_model(model, show_shapes=True, to_file='plot.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "# it is an important generator\n",
    "def data_generator(descriptions, features, tokenizer, max_length, n_step):\n",
    "    # loop until we finish training\n",
    "    # n_step means the procedure of each epoch\n",
    "    while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "        keys = list(descriptions.keys())\n",
    "        for i in range(0, len(keys), n_step):\n",
    "            Ximages, XSeq, y = list(), list(),list()\n",
    "            # maybe the last batch is less than the normal batch\n",
    "            for j in range(i, min(len(keys), i+n_step)):\n",
    "                image_id = keys[j]\n",
    "                # retrieve photo feature input\n",
    "                image = features[image_id][0]\n",
    "                # retrieve text input\n",
    "                desc = descriptions[image_id]\n",
    "                # generate input-output pairs\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    XSeq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "            # yield this batch of samples to the model\n",
    "            # normal result is array with [x_input,y_output] each \n",
    "            yield [[array(Ximages), array(XSeq)], array(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "# from the tokenizer id to the word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "# use the trained model and return the bleu value\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extract set generated and length 200 \n"
     ]
    }
   ],
   "source": [
    "dataset = seperate_dataset(img_dir, set_category, seed)\n",
    "# save_dataset(output_filename, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare_feature_desc(os.path.join(img_feature_dir, img_feature_file), os.path.join(clean_txt_dir, clean_txt_file))\n",
    "# train_desc and test_desc have the complete sentence of the photo description\n",
    "train_desc, test_desc = load_txt(os.path.join(clean_txt_dir, clean_txt_file), dataset)\n",
    "# the shape of the train_img is (1,7,7,512) ndarray\n",
    "train_img, test_img = load_image_feature(os.path.join(img_feature_dir, img_feature_file), dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate the train set\n",
    "# print(type(train_img))\n",
    "# print(list(train_img.keys()))\n",
    "# print(type(train_img['img_5519']))\n",
    "# print(train_img['img_5519'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some information about the trainging and test data set\n",
    "# print(' train_desc length %d test_desc length %d ' % (len(train_desc), len(test_desc)))\n",
    "# print(' train_img length %d test_img length %d ' % (len(train_img), len(test_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The maximum length of the description is 70 \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 512)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 70, 50)       42850       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65664       global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 70, 256)      314368      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 70, 128)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 70, 128)      32896       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 70, 256)      0           repeat_vector_1[0][0]            \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 500)          1514000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 500)          250500      lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 857)          429357      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,649,635\n",
      "Trainable params: 2,649,635\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      " - 176s - loss: 6.0323 - acc: 0.0357\n",
      "Epoch 2/50\n",
      " - 167s - loss: 5.6514 - acc: 0.0464\n",
      "Epoch 3/50\n",
      " - 180s - loss: 5.6029 - acc: 0.0488\n",
      "Epoch 4/50\n",
      " - 179s - loss: 5.5342 - acc: 0.0513\n",
      "Epoch 5/50\n",
      " - 178s - loss: 5.4918 - acc: 0.0489\n",
      "Epoch 6/50\n",
      " - 179s - loss: 5.4613 - acc: 0.0479\n",
      "Epoch 7/50\n",
      " - 184s - loss: 5.4160 - acc: 0.0497\n",
      "Epoch 8/50\n",
      " - 171s - loss: 5.3996 - acc: 0.0501\n",
      "Epoch 9/50\n",
      " - 172s - loss: 5.3671 - acc: 0.0493\n",
      "Epoch 10/50\n",
      " - 165s - loss: 5.3883 - acc: 0.0497\n",
      "Epoch 11/50\n",
      " - 168s - loss: 5.3597 - acc: 0.0522\n",
      "Epoch 12/50\n",
      " - 177s - loss: 5.3583 - acc: 0.0505\n",
      "Epoch 13/50\n",
      " - 169s - loss: 5.3409 - acc: 0.0482\n",
      "Epoch 14/50\n",
      " - 171s - loss: 5.3339 - acc: 0.0472\n",
      "Epoch 15/50\n",
      " - 165s - loss: 5.3332 - acc: 0.0525\n",
      "Epoch 16/50\n",
      " - 165s - loss: 5.3414 - acc: 0.0511\n",
      "Epoch 17/50\n",
      " - 165s - loss: 5.3279 - acc: 0.0583\n",
      "Epoch 18/50\n",
      " - 167s - loss: 5.3279 - acc: 0.0544\n",
      "Epoch 19/50\n",
      " - 170s - loss: 5.3225 - acc: 0.0567\n",
      "Epoch 20/50\n",
      " - 173s - loss: 5.3328 - acc: 0.0530\n",
      "Epoch 21/50\n",
      " - 170s - loss: 5.3085 - acc: 0.0530\n",
      "Epoch 22/50\n",
      " - 167s - loss: 5.3137 - acc: 0.0539\n",
      "Epoch 23/50\n",
      " - 165s - loss: 5.3104 - acc: 0.0495\n",
      "Epoch 24/50\n",
      " - 167s - loss: 5.2962 - acc: 0.0491\n",
      "Epoch 25/50\n",
      " - 169s - loss: 5.2931 - acc: 0.0495\n",
      "Epoch 26/50\n",
      " - 167s - loss: 5.3072 - acc: 0.0523\n",
      "Epoch 27/50\n",
      " - 173s - loss: 5.3059 - acc: 0.0524\n",
      "Epoch 28/50\n",
      " - 165s - loss: 5.3069 - acc: 0.0519\n",
      "Epoch 29/50\n",
      " - 168s - loss: 5.3351 - acc: 0.0562\n",
      "Epoch 30/50\n",
      " - 167s - loss: 5.3106 - acc: 0.0555\n",
      "Epoch 31/50\n",
      " - 169s - loss: 5.3141 - acc: 0.0527\n",
      "Epoch 32/50\n",
      " - 166s - loss: 5.2827 - acc: 0.0559\n",
      "Epoch 33/50\n",
      " - 172s - loss: 5.3028 - acc: 0.0569\n",
      "Epoch 34/50\n",
      " - 178s - loss: 5.2857 - acc: 0.0564\n",
      "Epoch 35/50\n",
      " - 171s - loss: 5.2507 - acc: 0.0539\n",
      "Epoch 36/50\n",
      " - 169s - loss: 5.2216 - acc: 0.0513\n",
      "Epoch 37/50\n",
      " - 54423s - loss: 5.1964 - acc: 0.0532\n",
      "Epoch 38/50\n",
      " - 203s - loss: 5.2027 - acc: 0.0522\n",
      "Epoch 39/50\n",
      " - 211s - loss: 5.1894 - acc: 0.0527\n",
      "Epoch 40/50\n",
      " - 189s - loss: 5.1996 - acc: 0.0539\n",
      "Epoch 41/50\n",
      " - 183s - loss: 5.2043 - acc: 0.0537\n",
      "Epoch 42/50\n",
      " - 185s - loss: 5.2004 - acc: 0.0522\n",
      "Epoch 43/50\n",
      " - 169s - loss: 5.1637 - acc: 0.0548\n",
      "Epoch 44/50\n",
      " - 172s - loss: 5.1554 - acc: 0.0492\n",
      "Epoch 45/50\n",
      " - 184s - loss: 5.1949 - acc: 0.0511\n",
      "Epoch 46/50\n",
      " - 185s - loss: 5.1790 - acc: 0.0542\n",
      "Epoch 47/50\n",
      " - 183s - loss: 5.1609 - acc: 0.0518\n",
      "Epoch 48/50\n",
      " - 183s - loss: 5.1482 - acc: 0.0536\n",
      "Epoch 49/50\n",
      " - 174s - loss: 5.1444 - acc: 0.0511\n",
      "Epoch 50/50\n",
      " - 177s - loss: 5.1332 - acc: 0.0539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programfiles\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---1: train=0.068699 test=0.067455\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 512)          0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 70, 50)       42850       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          65664       global_max_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 70, 256)      314368      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 70, 128)      0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 70, 128)      32896       lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 70, 256)      0           repeat_vector_2[0][0]            \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 500)          1514000     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 500)          250500      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 857)          429357      dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,649,635\n",
      "Trainable params: 2,649,635\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      " - 186s - loss: 6.0039 - acc: 0.0422\n",
      "Epoch 2/50\n",
      " - 180s - loss: 5.6448 - acc: 0.0501\n",
      "Epoch 3/50\n",
      " - 170s - loss: 5.5922 - acc: 0.0493\n",
      "Epoch 4/50\n",
      " - 168s - loss: 5.5267 - acc: 0.0522\n",
      "Epoch 5/50\n",
      " - 177s - loss: 5.4832 - acc: 0.0497\n",
      "Epoch 6/50\n",
      " - 186s - loss: 5.4863 - acc: 0.0532\n",
      "Epoch 7/50\n",
      " - 178s - loss: 5.4225 - acc: 0.0542\n",
      "Epoch 8/50\n",
      " - 172s - loss: 5.3953 - acc: 0.0530\n",
      "Epoch 9/50\n",
      " - 175s - loss: 5.3693 - acc: 0.0513\n",
      "Epoch 10/50\n",
      " - 177s - loss: 5.3525 - acc: 0.0537\n",
      "Epoch 11/50\n",
      " - 179s - loss: 5.3601 - acc: 0.0519\n",
      "Epoch 12/50\n",
      " - 175s - loss: 5.3517 - acc: 0.0500\n",
      "Epoch 13/50\n",
      " - 180s - loss: 5.3194 - acc: 0.0491\n",
      "Epoch 14/50\n",
      " - 189s - loss: 5.3237 - acc: 0.0560\n",
      "Epoch 15/50\n",
      " - 185s - loss: 5.3110 - acc: 0.0465\n",
      "Epoch 16/50\n",
      " - 185s - loss: 5.3065 - acc: 0.0511\n",
      "Epoch 17/50\n",
      " - 176s - loss: 5.2946 - acc: 0.0505\n",
      "Epoch 18/50\n",
      " - 174s - loss: 5.2902 - acc: 0.0520\n",
      "Epoch 19/50\n",
      " - 172s - loss: 5.2869 - acc: 0.0497\n",
      "Epoch 20/50\n",
      " - 172s - loss: 5.2690 - acc: 0.0455\n",
      "Epoch 21/50\n",
      " - 173s - loss: 5.2516 - acc: 0.0566\n",
      "Epoch 22/50\n",
      " - 175s - loss: 5.2593 - acc: 0.0515\n",
      "Epoch 23/50\n",
      " - 173s - loss: 5.2618 - acc: 0.0503\n",
      "Epoch 24/50\n",
      " - 172s - loss: 5.2579 - acc: 0.0516\n",
      "Epoch 25/50\n",
      " - 172s - loss: 5.2236 - acc: 0.0512\n",
      "Epoch 26/50\n",
      " - 179s - loss: 5.2228 - acc: 0.0539\n",
      "Epoch 27/50\n",
      " - 173s - loss: 5.2153 - acc: 0.0567\n",
      "Epoch 28/50\n",
      " - 185s - loss: 5.2064 - acc: 0.0528\n",
      "Epoch 29/50\n",
      " - 185s - loss: 5.2256 - acc: 0.0542\n",
      "Epoch 30/50\n",
      " - 179s - loss: 5.2392 - acc: 0.0524\n",
      "Epoch 31/50\n",
      " - 179s - loss: 5.1978 - acc: 0.0511\n",
      "Epoch 32/50\n",
      " - 177s - loss: 5.2111 - acc: 0.0470\n",
      "Epoch 33/50\n",
      " - 174s - loss: 5.2421 - acc: 0.0454\n",
      "Epoch 34/50\n",
      " - 172s - loss: 5.2398 - acc: 0.0534\n",
      "Epoch 35/50\n",
      " - 172s - loss: 5.2399 - acc: 0.0487\n",
      "Epoch 36/50\n",
      " - 172s - loss: 5.1897 - acc: 0.0526\n",
      "Epoch 37/50\n",
      " - 175s - loss: 5.1796 - acc: 0.0533\n",
      "Epoch 38/50\n",
      " - 182s - loss: 5.1825 - acc: 0.0527\n",
      "Epoch 39/50\n",
      " - 182s - loss: 5.1563 - acc: 0.0564\n",
      "Epoch 40/50\n",
      " - 177s - loss: 5.1601 - acc: 0.0519\n",
      "Epoch 41/50\n",
      " - 172s - loss: 5.1628 - acc: 0.0527\n",
      "Epoch 42/50\n",
      " - 172s - loss: 5.1387 - acc: 0.0518\n",
      "Epoch 43/50\n",
      " - 172s - loss: 5.1301 - acc: 0.0512\n",
      "Epoch 44/50\n",
      " - 172s - loss: 5.1126 - acc: 0.0554\n",
      "Epoch 45/50\n",
      " - 175s - loss: 5.1152 - acc: 0.0521\n",
      "Epoch 46/50\n",
      " - 172s - loss: 5.0850 - acc: 0.0558\n",
      "Epoch 47/50\n",
      " - 173s - loss: 5.0823 - acc: 0.0544\n",
      "Epoch 48/50\n",
      " - 178s - loss: 5.0678 - acc: 0.0513\n",
      "Epoch 49/50\n",
      " - 173s - loss: 5.0626 - acc: 0.0538\n",
      "Epoch 50/50\n",
      " - 173s - loss: 5.0608 - acc: 0.0537\n",
      "---2: train=0.085476 test=0.080101\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 512)          0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 70, 50)       42850       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          65664       global_max_pooling2d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 70, 256)      314368      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_3 (RepeatVector)  (None, 70, 128)      0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 70, 128)      32896       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 70, 256)      0           repeat_vector_3[0][0]            \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 500)          1514000     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 500)          250500      lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 857)          429357      dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,649,635\n",
      "Trainable params: 2,649,635\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 178s - loss: 6.0147 - acc: 0.0460\n",
      "Epoch 2/50\n",
      " - 174s - loss: 5.6707 - acc: 0.0470\n",
      "Epoch 3/50\n",
      " - 175s - loss: 5.6263 - acc: 0.0435\n",
      "Epoch 4/50\n",
      " - 177s - loss: 5.5943 - acc: 0.0486\n",
      "Epoch 5/50\n",
      " - 185s - loss: 5.5437 - acc: 0.0483\n",
      "Epoch 6/50\n",
      " - 182s - loss: 5.4842 - acc: 0.0519\n",
      "Epoch 7/50\n",
      " - 180s - loss: 5.4466 - acc: 0.0521\n",
      "Epoch 8/50\n",
      " - 175s - loss: 5.4127 - acc: 0.0528\n",
      "Epoch 9/50\n",
      " - 175s - loss: 5.3820 - acc: 0.0541\n",
      "Epoch 10/50\n",
      " - 177s - loss: 5.3610 - acc: 0.0538\n",
      "Epoch 11/50\n",
      " - 175s - loss: 5.3475 - acc: 0.0515\n",
      "Epoch 12/50\n",
      " - 180s - loss: 5.3700 - acc: 0.0468\n",
      "Epoch 13/50\n",
      " - 182s - loss: 5.3442 - acc: 0.0502\n",
      "Epoch 14/50\n",
      " - 177s - loss: 5.3194 - acc: 0.0538\n",
      "Epoch 15/50\n",
      " - 178s - loss: 5.3209 - acc: 0.0530\n",
      "Epoch 16/50\n",
      " - 188s - loss: 5.3072 - acc: 0.0487\n",
      "Epoch 17/50\n",
      " - 174s - loss: 5.2941 - acc: 0.0467\n",
      "Epoch 18/50\n",
      " - 171s - loss: 5.2825 - acc: 0.0499\n",
      "Epoch 19/50\n",
      " - 175s - loss: 5.2703 - acc: 0.0447\n",
      "Epoch 20/50\n",
      " - 173s - loss: 5.2703 - acc: 0.0500\n",
      "Epoch 21/50\n",
      " - 174s - loss: 5.2670 - acc: 0.0507\n",
      "Epoch 22/50\n",
      " - 173s - loss: 5.2539 - acc: 0.0500\n",
      "Epoch 23/50\n",
      " - 173s - loss: 5.2577 - acc: 0.0546\n",
      "Epoch 24/50\n",
      " - 171s - loss: 5.2500 - acc: 0.0494\n",
      "Epoch 25/50\n",
      " - 180s - loss: 5.2452 - acc: 0.0514\n",
      "Epoch 26/50\n",
      " - 178s - loss: 5.2361 - acc: 0.0534\n",
      "Epoch 27/50\n",
      " - 175s - loss: 5.2247 - acc: 0.0551\n",
      "Epoch 28/50\n",
      " - 189s - loss: 5.2065 - acc: 0.0511\n",
      "Epoch 29/50\n",
      " - 174s - loss: 5.2067 - acc: 0.0522\n",
      "Epoch 30/50\n",
      " - 178s - loss: 5.1881 - acc: 0.0521\n",
      "Epoch 31/50\n",
      " - 186s - loss: 5.2087 - acc: 0.0516\n",
      "Epoch 32/50\n",
      " - 181s - loss: 5.2073 - acc: 0.0493\n",
      "Epoch 33/50\n",
      " - 187s - loss: 5.2018 - acc: 0.0481\n",
      "Epoch 34/50\n",
      " - 186s - loss: 5.1681 - acc: 0.0539\n",
      "Epoch 35/50\n",
      " - 176s - loss: 5.1774 - acc: 0.0547\n",
      "Epoch 36/50\n",
      " - 180s - loss: 5.1699 - acc: 0.0476\n",
      "Epoch 37/50\n",
      " - 174s - loss: 5.1773 - acc: 0.0545\n",
      "Epoch 38/50\n",
      " - 171s - loss: 5.2050 - acc: 0.0479\n",
      "Epoch 39/50\n",
      " - 171s - loss: 5.2110 - acc: 0.0491\n",
      "Epoch 40/50\n",
      " - 175s - loss: 5.1817 - acc: 0.0534\n",
      "Epoch 41/50\n",
      " - 171s - loss: 5.1645 - acc: 0.0536\n",
      "Epoch 42/50\n",
      " - 174s - loss: 5.1631 - acc: 0.0531\n",
      "Epoch 43/50\n",
      " - 172s - loss: 5.1660 - acc: 0.0548\n",
      "Epoch 44/50\n",
      " - 185s - loss: 5.1493 - acc: 0.0459\n",
      "Epoch 45/50\n",
      " - 172s - loss: 5.1457 - acc: 0.0492\n",
      "Epoch 46/50\n",
      " - 174s - loss: 5.1324 - acc: 0.0497\n",
      "Epoch 47/50\n",
      " - 173s - loss: 5.1320 - acc: 0.0546\n",
      "Epoch 48/50\n"
     ]
    }
   ],
   "source": [
    "# begin to train the model\n",
    "# get the tokenizer\n",
    "# just use the trainig desc to get the tokenizer\n",
    "# and for the test desc still use the trainig tokenizer\n",
    "tokenizer = create_tokenizer(train_desc)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# get the maximum length of the descriptions\n",
    "max_length = max([len(item.split()) for item in list(train_desc.values())])\n",
    "print(' The maximum length of the description is %d ' % max_length)\n",
    "\n",
    "# define the experiment\n",
    "model_name = 'basiccaptionmodel'\n",
    "verbose = 2\n",
    "# set the iterate times\n",
    "n_epochs = 50\n",
    "# set the picture update number\n",
    "n_photos_per_update = 2\n",
    "# calculate the batches per epoch\n",
    "n_batches_per_epoch = int(len(dataset['train']) / n_photos_per_update)\n",
    "# set the repeat number there is no k fold cross validation\n",
    "# in this training procedure there is no cross validation\n",
    "# in this part normally the repeats will be at list 30 times but consider the time\n",
    "# just 3 times for the pre test\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_desc, train_img, tokenizer, max_length, n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_desc, train_img, tokenizer, max_length)\n",
    "    # evaluate the model on test data\n",
    "    test_score = evaluate_model(model, test_desc, test_img, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('---%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(' current training result ')\n",
    "print(df.describe())\n",
    "df.to_csv(os.path.join(output_dir, model_name+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}