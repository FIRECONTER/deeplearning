{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: \\n    1) Use the pre handled data(image description and image features to train the caption geneartion model\\n    2) firstly quick choose a model to train\\nAuthor: allocator\\n'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Description: \n",
    "    1) Use the pre handled data(image description and image features to train the caption geneartion model\n",
    "    2) firstly quick choose a model to train\n",
    "Author: allocator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy.random as rd\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dir = '../data/img'\n",
    "img_feature_dir = '../data/img_feature'\n",
    "img_feature_file = 'image_features.h5'\n",
    "clean_txt_dir = '../data/clean_txt'\n",
    "clean_txt_file = 'image_descs.json'\n",
    "set_category = 200\n",
    "seed = 10\n",
    "output_dir = '../data/res'\n",
    "output_filename = 'development_dataset_id.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare the image feature list and the image desc list\n",
    "def compare_feature_desc(feature_file, desc_file):\n",
    "    \"\"\"Compare the image feature and desc.\"\"\"\n",
    "    image_features = h5py.File(feature_file, 'r')\n",
    "    image_descs = json.load(open(desc_file, 'r'))\n",
    "    image_feature_keys = dict(image_features.keys())\n",
    "    image_descs_keys = image_descs.keys()\n",
    "    print(' current image_feature_keys')\n",
    "    print(image_feature_keys)\n",
    "    print(' current image_descs_keys')\n",
    "    print(image_descs_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first seperate the data set\n",
    "def seperate_dataset(img_dir, category, seed):\n",
    "    \"\"\"Generate the development set to quickly choose the model and configuration\"\"\"\n",
    "    img_list = os.listdir(img_dir)\n",
    "    # random select the train set and test set from the image list\n",
    "    img_list = [item.split('.')[0] for item in img_list]\n",
    "    img_size = len(img_list)\n",
    "    extract_set = set()\n",
    "    dataset = {}\n",
    "    rd.seed(seed)\n",
    "    while len(extract_set) < category:\n",
    "        curr_id = rd.randint(img_size)\n",
    "        curr_item = img_list[curr_id]\n",
    "        if curr_item not in extract_set:\n",
    "            extract_set.add(curr_item)\n",
    "    print(' extract set generated and length %d ' % len(extract_set))\n",
    "    set_len = int(category/2)\n",
    "    dataset['train'] = list(extract_set)[:set_len]\n",
    "    dataset['test'] = list(extract_set)[set_len:]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the dataset id\n",
    "def save_dataset(filename, dataset):\n",
    "    \"\"\"Save the dataset.\"\"\"\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    json.dump(dataset, open(file_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_txt(filename, dataset):\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    train_id_list = dataset['train']\n",
    "    test_id_list = dataset['test']\n",
    "    image_descs = json.load(open(filename, 'r'))\n",
    "    for item in train_id_list:\n",
    "        train_set[item] = 'startseq ' + ' '.join(image_descs[item]) + ' endseq'\n",
    "    for item in test_id_list:\n",
    "        test_set[item] = 'startseq ' + ' '.join(image_descs[item]) + ' endseq'\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the features about the images\n",
    "def load_image_feature(filename, dataset):\n",
    "    \"\"\"Load the image feature about the train and test image id list.\"\"\"\n",
    "    image_features = h5py.File(filename, 'r')\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    train_id_list = dataset['train']\n",
    "    test_id_list = dataset['test']\n",
    "    for item in train_id_list:\n",
    "        train_set[item] = np.array(image_features[item])\n",
    "    for item in test_id_list:\n",
    "        test_set[item] = np.array(image_features[item])\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to encode the descriptions before training\n",
    "# from words to unique integer values\n",
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"Encode the descriptions to numbers for model training.\"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    lines = list(descriptions.values())\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the important generate training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the sequence of the images input sequences and output words for an image\n",
    "def create_sequences(tokenizer, image_desc, image, max_length):\n",
    "    ximage, xseqs, y = list(), list(), list()\n",
    "    # encode the description with integer\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    sequence = tokenizer.texts_to_sequences([image_desc])[0]\n",
    "    # splite current sequence to multiple x,y pairs\n",
    "    for i in range(1, len(sequence)):\n",
    "        # split the sequence\n",
    "        input_seq, output_seq = sequence[:i], sequence[i]\n",
    "        # \n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
    "        # \n",
    "        output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "        ximage.append(image)\n",
    "        xseqs.append(input_seq)\n",
    "        y.append(output_seq)\n",
    "    return [ximage, xseqs, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some explain about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# begin to fit the model\n",
    "def define_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='plot.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length, n_step):\n",
    "    # loop until we finish training\n",
    "    while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "        keys = list(descriptions.keys())\n",
    "        for i in range(0, len(keys), n_step):\n",
    "            Ximages, XSeq, y = list(), list(),list()\n",
    "            for j in range(i, min(len(keys), i+n_step)):\n",
    "                image_id = keys[j]\n",
    "                # retrieve photo feature input\n",
    "                image = features[image_id][0]\n",
    "                # retrieve text input\n",
    "                desc = descriptions[image_id]\n",
    "                # generate input-output pairs\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    XSeq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "            # yield this batch of samples to the model\n",
    "            yield [[array(Ximages), array(XSeq)], array(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extract set generated and length 200 \n"
     ]
    }
   ],
   "source": [
    "dataset = seperate_dataset(img_dir, set_category, seed)\n",
    "# save_dataset(output_filename, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_feature_desc(os.path.join(img_feature_dir, img_feature_file), os.path.join(clean_txt_dir, clean_txt_file))\n",
    "train_desc, test_desc = load_txt(os.path.join(clean_txt_dir, clean_txt_file), dataset)\n",
    "train_img, test_img = load_image_feature(os.path.join(img_feature_dir, img_feature_file), dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train_desc length 100 test_desc length 100 \n",
      " train_img length 100 test_img length 100 \n"
     ]
    }
   ],
   "source": [
    "# some information about the trainging and test data set\n",
    "# print(' train_desc length %d test_desc length %d ' % (len(train_desc), len(test_desc)))\n",
    "# print(' train_img length %d test_img length %d ' % (len(train_img), len(test_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the tokenizer\n",
    "train_desc_tokenizer = create_tokenizer(train_desc)\n",
    "# print(' training desc vocabulary is %d ' len(train_desc_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
