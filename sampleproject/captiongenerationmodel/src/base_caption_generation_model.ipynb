{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: \\n    1) Use the pre handled data(image description and image features to train the caption geneartion model\\n    2) firstly quick choose a model to train\\nAuthor: allocator\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Description: \n",
    "    1) Use the pre handled data(image description and image features to train the caption geneartion model\n",
    "    2) firstly quick choose a model to train\n",
    "Author: allocator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "import numpy.random as rd\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dir = '../data/img'\n",
    "img_feature_dir = '../data/img_feature'\n",
    "img_feature_file = 'image_features.h5'\n",
    "clean_txt_dir = '../data/clean_txt'\n",
    "clean_txt_file = 'image_descs.json'\n",
    "set_category = 200\n",
    "seed = 10\n",
    "output_dir = '../data/res'\n",
    "output_filename = 'development_dataset_id.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare the image feature list and the image desc list\n",
    "def compare_feature_desc(feature_file, desc_file):\n",
    "    \"\"\"Compare the image feature and desc.\"\"\"\n",
    "    image_features = h5py.File(feature_file, 'r')\n",
    "    image_descs = json.load(open(desc_file, 'r'))\n",
    "    image_feature_keys = dict(image_features.keys())\n",
    "    image_descs_keys = image_descs.keys()\n",
    "    print(' current image_feature_keys')\n",
    "    print(image_feature_keys)\n",
    "    print(' current image_descs_keys')\n",
    "    print(image_descs_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first seperate the data set\n",
    "def seperate_dataset(img_dir, category, seed):\n",
    "    \"\"\"Generate the development set to quickly choose the model and configuration\"\"\"\n",
    "    img_list = os.listdir(img_dir)\n",
    "    # random select the train set and test set from the image list\n",
    "    img_list = [item.split('.')[0] for item in img_list]\n",
    "    img_size = len(img_list)\n",
    "    extract_set = set()\n",
    "    dataset = {}\n",
    "    rd.seed(seed)\n",
    "    while len(extract_set) < category:\n",
    "        curr_id = rd.randint(img_size)\n",
    "        curr_item = img_list[curr_id]\n",
    "        if curr_item not in extract_set:\n",
    "            extract_set.add(curr_item)\n",
    "    print(' extract set generated and length %d ' % len(extract_set))\n",
    "    set_len = int(category/2)\n",
    "    dataset['train'] = list(extract_set)[:set_len]\n",
    "    dataset['test'] = list(extract_set)[set_len:]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the dataset id\n",
    "def save_dataset(filename, dataset):\n",
    "    \"\"\"Save the dataset.\"\"\"\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    json.dump(dataset, open(file_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_txt(filename, dataset):\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    train_id_list = dataset['train']\n",
    "    test_id_list = dataset['test']\n",
    "    image_descs = json.load(open(filename, 'r'))\n",
    "    for item in train_id_list:\n",
    "        train_set[item] = 'startseq ' + ' '.join(image_descs[item]) + ' endseq'\n",
    "    for item in test_id_list:\n",
    "        test_set[item] = 'startseq ' + ' '.join(image_descs[item]) + ' endseq'\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the features about the images\n",
    "def load_image_feature(filename, dataset):\n",
    "    \"\"\"Load the image feature about the train and test image id list.\"\"\"\n",
    "    image_features = h5py.File(filename, 'r')\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    train_id_list = dataset['train']\n",
    "    test_id_list = dataset['test']\n",
    "    for item in train_id_list:\n",
    "        train_set[item] = np.array(image_features[item])\n",
    "    for item in test_id_list:\n",
    "        test_set[item] = np.array(image_features[item])\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to encode the descriptions before training\n",
    "# from words to unique integer values\n",
    "def create_tokenizer(descriptions):\n",
    "    \"\"\"Encode the descriptions to numbers for model training.\"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    # each line contains the the description sentence about the image\n",
    "    lines = list(descriptions.values())\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the important generate training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the sequence of the images input sequences and output words for an image\n",
    "def create_sequences(tokenizer, image_desc, image, max_length):\n",
    "    ximage, xseqs, y = list(), list(), list()\n",
    "    # encode the description with integer why pluse one\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    sequence = tokenizer.texts_to_sequences([image_desc])[0]\n",
    "    # splite current sequence to multiple x,y pairs\n",
    "    for i in range(1, len(sequence)):\n",
    "        # split the sequence\n",
    "        input_seq, output_seq = sequence[:i], sequence[i]\n",
    "        # pad input sequence to make the sequence same length\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
    "        # encode output make the output as a categorical list\n",
    "        output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "        ximage.append(image)\n",
    "        xseqs.append(input_seq)\n",
    "        y.append(output_seq)\n",
    "    # each picture contains seqs images output are list\n",
    "    return [ximage, xseqs, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some explain about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# begin to fit the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    # image feature map\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs image descs sequence and image features\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # plot_model(model, show_shapes=True, to_file='plot.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "# it is an important generator\n",
    "def data_generator(descriptions, features, tokenizer, max_length, n_step):\n",
    "    # loop until we finish training\n",
    "    while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "        keys = list(descriptions.keys())\n",
    "        for i in range(0, len(keys), n_step):\n",
    "            Ximages, XSeq, y = list(), list(),list()\n",
    "            for j in range(i, min(len(keys), i+n_step)):\n",
    "                image_id = keys[j]\n",
    "                # retrieve photo feature input\n",
    "                image = features[image_id][0]\n",
    "                # retrieve text input\n",
    "                desc = descriptions[image_id]\n",
    "                # generate input-output pairs\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    XSeq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "            # yield this batch of samples to the model\n",
    "            yield [[array(Ximages), array(XSeq)], array(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extract set generated and length 200 \n"
     ]
    }
   ],
   "source": [
    "dataset = seperate_dataset(img_dir, set_category, seed)\n",
    "# save_dataset(output_filename, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare_feature_desc(os.path.join(img_feature_dir, img_feature_file), os.path.join(clean_txt_dir, clean_txt_file))\n",
    "train_desc, test_desc = load_txt(os.path.join(clean_txt_dir, clean_txt_file), dataset)\n",
    "train_img, test_img = load_image_feature(os.path.join(img_feature_dir, img_feature_file), dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some information about the trainging and test data set\n",
    "# print(' train_desc length %d test_desc length %d ' % (len(train_desc), len(test_desc)))\n",
    "# print(' train_img length %d test_img length %d ' % (len(train_img), len(test_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The maximum length of the description is 70 \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 512)          0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 70, 50)       43300       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          65664       global_max_pooling2d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 70, 256)      314368      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_3 (RepeatVector)  (None, 70, 128)      0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 70, 128)      32896       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 70, 256)      0           repeat_vector_3[0][0]            \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 500)          1514000     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 500)          250500      lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 866)          433866      dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,654,594\n",
      "Trainable params: 2,654,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      " - 178s - loss: 6.0326 - acc: 0.0478\n",
      "Epoch 2/50\n",
      " - 185s - loss: 5.6794 - acc: 0.0505\n",
      "Epoch 3/50\n",
      " - 168s - loss: 5.6563 - acc: 0.0517\n",
      "Epoch 4/50\n",
      " - 175s - loss: 5.6468 - acc: 0.0515\n",
      "Epoch 5/50\n",
      " - 177s - loss: 5.6009 - acc: 0.0535\n",
      "Epoch 6/50\n",
      " - 170s - loss: 5.5705 - acc: 0.0541\n",
      "Epoch 7/50\n",
      " - 171s - loss: 5.5141 - acc: 0.0543\n",
      "Epoch 8/50\n",
      " - 170s - loss: 5.4500 - acc: 0.0522\n",
      "Epoch 9/50\n",
      " - 180s - loss: 5.4189 - acc: 0.0620\n",
      "Epoch 10/50\n",
      " - 183s - loss: 5.3813 - acc: 0.0505\n",
      "Epoch 11/50\n",
      " - 183s - loss: 5.3678 - acc: 0.0479\n",
      "Epoch 12/50\n",
      " - 176s - loss: 5.3632 - acc: 0.0550\n",
      "Epoch 13/50\n",
      " - 174s - loss: 5.3350 - acc: 0.0540\n",
      "Epoch 14/50\n",
      " - 182s - loss: 5.3162 - acc: 0.0623\n",
      "Epoch 15/50\n",
      " - 176s - loss: 5.3135 - acc: 0.0564\n",
      "Epoch 16/50\n",
      " - 178s - loss: 5.3297 - acc: 0.0618\n",
      "Epoch 17/50\n",
      " - 173s - loss: 5.2973 - acc: 0.0582\n",
      "Epoch 18/50\n",
      " - 172s - loss: 5.2904 - acc: 0.0531\n",
      "Epoch 19/50\n",
      " - 171s - loss: 5.2801 - acc: 0.0548\n",
      "Epoch 20/50\n",
      " - 172s - loss: 5.2564 - acc: 0.0589\n",
      "Epoch 21/50\n",
      " - 171s - loss: 5.2419 - acc: 0.0517\n",
      "Epoch 22/50\n",
      " - 190s - loss: 5.2333 - acc: 0.0525\n",
      "Epoch 23/50\n",
      " - 178s - loss: 5.2304 - acc: 0.0537\n",
      "Epoch 24/50\n",
      " - 166s - loss: 5.2268 - acc: 0.0547\n",
      "Epoch 25/50\n",
      " - 169s - loss: 5.2481 - acc: 0.0544\n",
      "Epoch 26/50\n",
      " - 164s - loss: 5.2103 - acc: 0.0611\n",
      "Epoch 27/50\n",
      " - 165s - loss: 5.1903 - acc: 0.0580\n",
      "Epoch 28/50\n",
      " - 165s - loss: 5.1891 - acc: 0.0543\n",
      "Epoch 29/50\n",
      " - 167s - loss: 5.2091 - acc: 0.0601\n",
      "Epoch 30/50\n",
      " - 166s - loss: 5.1761 - acc: 0.0545\n",
      "Epoch 31/50\n",
      " - 165s - loss: 5.1725 - acc: 0.0613\n",
      "Epoch 32/50\n",
      " - 165s - loss: 5.1606 - acc: 0.0564\n",
      "Epoch 33/50\n",
      " - 168s - loss: 5.1417 - acc: 0.0590\n",
      "Epoch 34/50\n",
      " - 165s - loss: 5.1469 - acc: 0.0583\n",
      "Epoch 35/50\n",
      " - 169s - loss: 5.1857 - acc: 0.0560\n",
      "Epoch 36/50\n",
      " - 169s - loss: 5.1789 - acc: 0.0575\n",
      "Epoch 37/50\n",
      " - 167s - loss: 5.1473 - acc: 0.0593\n",
      "Epoch 38/50\n",
      " - 166s - loss: 5.1278 - acc: 0.0587\n",
      "Epoch 39/50\n",
      " - 166s - loss: 5.1219 - acc: 0.0614\n",
      "Epoch 40/50\n",
      " - 166s - loss: 5.1175 - acc: 0.0606\n",
      "Epoch 41/50\n",
      " - 172s - loss: 5.1204 - acc: 0.0566\n",
      "Epoch 42/50\n",
      " - 177s - loss: 5.1049 - acc: 0.0622\n",
      "Epoch 43/50\n",
      " - 168s - loss: 5.1053 - acc: 0.0583\n",
      "Epoch 44/50\n",
      " - 169s - loss: 5.0933 - acc: 0.0624\n",
      "Epoch 45/50\n",
      " - 166s - loss: 5.1025 - acc: 0.0595\n",
      "Epoch 46/50\n",
      " - 175s - loss: 5.1099 - acc: 0.0569\n",
      "Epoch 47/50\n",
      " - 170s - loss: 5.0791 - acc: 0.0640\n",
      "Epoch 48/50\n",
      " - 169s - loss: 5.0752 - acc: 0.0611\n",
      "Epoch 49/50\n",
      " - 165s - loss: 5.0877 - acc: 0.0632\n",
      "Epoch 50/50\n",
      " - 165s - loss: 5.0611 - acc: 0.0590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programfiles\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---1: train=0.073921 test=0.061608\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_4 (GlobalM (None, 512)          0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 70, 50)       43300       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          65664       global_max_pooling2d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 70, 256)      314368      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_4 (RepeatVector)  (None, 70, 128)      0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 70, 128)      32896       lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 70, 256)      0           repeat_vector_4[0][0]            \n",
      "                                                                 time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 500)          1514000     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 500)          250500      lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 866)          433866      dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,654,594\n",
      "Trainable params: 2,654,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      " - 165s - loss: 6.0443 - acc: 0.0443\n",
      "Epoch 2/50\n",
      " - 47210s - loss: 5.6904 - acc: 0.0485\n",
      "Epoch 3/50\n",
      " - 198s - loss: 5.6719 - acc: 0.0519\n",
      "Epoch 4/50\n"
     ]
    }
   ],
   "source": [
    "# begin to train the model\n",
    "# get the tokenizer\n",
    "tokenizer = create_tokenizer(train_desc)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# get the maximum length of the description\n",
    "max_length = max([len(item.split()) for item in list(train_desc.values())])\n",
    "print(' The maximum length of the description is %d ' % max_length)\n",
    "\n",
    "# define the experiment\n",
    "model_name = 'basiccaptionmodel'\n",
    "verbose = 2\n",
    "# set the iterate times\n",
    "n_epochs = 50\n",
    "# set the picture update number\n",
    "n_photos_per_update = 2\n",
    "# calculate the batches per epoch\n",
    "n_batches_per_epoch = int(len(dataset['train']) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_desc, train_img, tokenizer, max_length, n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_desc, train_img, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_desc, test_img, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('---%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(' current training result ')\n",
    "print(df.describe())\n",
    "df.to_csv(os.path.join(output_dir, model_name+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
